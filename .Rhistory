set.seed(234)
folds = bootstraps(train)
library(splines)
# Recipe with polynomial features and interaction
muhat_spline_rec <- recipe(muhat ~ X1 + X2, data = train) %>%
step_poly(X1, degree = tune("x1_degree")) %>%  # Polynomial for X1
step_poly(X2, degree = tune("x2_degree")) %>%  # Polynomial for X2
step_interact(terms = ~ starts_with("X1"):starts_with("X2"))  # Interaction
# Model specification
lm_spec <- linear_reg() %>%
set_engine("lm")  # Use glmnet or stan if needed
# Workflow
muhat_spline_wf <- workflow() %>%
add_recipe(muhat_spline_rec) %>%
add_model(lm_spec)
# Tuning grid
spline_grid <- expand.grid(
x1_degree = c(1:10),
x2_degree = c(1:10)
)
# Register parallel backend
doParallel::registerDoParallel()
# Control for saving predictions
save_preds <- control_grid(save_pred = TRUE)
# Tuning
spline_mu <- tune_grid(
muhat_spline_wf,
resamples = folds,
grid = spline_grid,
control = save_preds
)
# Stop parallel processing
doParallel::stopImplicitCluster()
# Evaluate Results
collect_metrics(spline_mu)
show_best(spline_rs, metric = "rmse")
#Best_muhat = select_best()
Best_muhat = select_best(spline_mu, metric = 'rmse')
Best_muhat = select_best(spline_mu, metric = 'rmse')
Best_muhat
Final_mu = finalize_workflow(muhat_spline_wf, best_muhat )
Final_mu = finalize_workflow(muhat_spline_wf, Best_muhat )
Final_mu
mu_spline_test = last_fit(Final_mu, split)
collect_metrics(mu_spline_test)
1.1085447^2
#setup multitheading
library(doParallel)
cores = detectCores()
library(dplyr)
library(tidymodels)
library(usemodels)
## Assume you save the training data in the folder "C:/temp" in your local laptop
traindata <- read.table(file = "C:/temp/7406train.csv", sep=",");
dim(traindata);
## dim=10000*202
## The first two columns are X1 and X2 values, and the last 200 columns are the Y valus
### Some example plots for exploratory data analysis
### please feel free to add more exploratory analysis
testX  <- read.table(file = "C:/temp/7406test.csv", sep=",")
names(traindata)[names(traindata) == 'V1'] = 'X1'
names(traindata)[names(traindata) == 'V2'] = 'X2'
names(testX)[names(testX) == 'V1'] = 'X1'
names(testX)[names(testX) == 'V2'] = 'X2'
X1 <- traindata[,1];
X2 <- traindata[,2];
## note that muhat = E(Y) and Vhat = Var(Y)
muhat <- apply(traindata[,3:202], 1, mean);
Vhat  <- apply(traindata[,3:202], 1, var)
data0 = data.frame(X1 = X1, X2=X2, muhat = muhat, Vhat = Vhat)
#tidy models learning stuff
set.seed(123)
#splitting data
split = initial_split(data0)
train = training(split)
test = testing(split)
set.seed(234)
folds = bootstraps(train)
library(splines)
# Recipe with polynomial features and interaction
muhat_spline_rec <- recipe(muhat ~ X1 + X2, data = train) %>%
step_poly(X1, degree = tune("x1_degree")) %>%  # Polynomial for X1
step_poly(X2, degree = tune("x2_degree")) %>%  # Polynomial for X2
step_interact(terms = ~ starts_with("X1"):starts_with("X2"))  # Interaction
# Model specification
lm_spec <- linear_reg() %>%
set_engine("glm")  # Use glmnet or stan if needed
# Workflow
muhat_spline_wf <- workflow() %>%
add_recipe(muhat_spline_rec) %>%
add_model(lm_spec)
# Tuning grid
spline_grid <- expand.grid(
x1_degree = c(1:15),
x2_degree = c(1:15)
)
# Register parallel backend
doParallel::registerDoParallel()
# Control for saving predictions
save_preds <- control_grid(save_pred = TRUE)
# Tuning
spline_mu <- tune_grid(
muhat_spline_wf,
resamples = folds,
grid = spline_grid,
control = save_preds
)
# Stop parallel processing
doParallel::stopImplicitCluster()
# Evaluate Results
collect_metrics(spline_mu)
show_best(spline_rs, metric = "rmse")
#setup multitheading
library(doParallel)
cores = detectCores()
library(dplyr)
library(tidymodels)
library(usemodels)
## Assume you save the training data in the folder "C:/temp" in your local laptop
traindata <- read.table(file = "C:/temp/7406train.csv", sep=",");
dim(traindata);
## dim=10000*202
## The first two columns are X1 and X2 values, and the last 200 columns are the Y valus
### Some example plots for exploratory data analysis
### please feel free to add more exploratory analysis
testX  <- read.table(file = "C:/temp/7406test.csv", sep=",")
names(traindata)[names(traindata) == 'V1'] = 'X1'
names(traindata)[names(traindata) == 'V2'] = 'X2'
names(testX)[names(testX) == 'V1'] = 'X1'
names(testX)[names(testX) == 'V2'] = 'X2'
X1 <- traindata[,1];
X2 <- traindata[,2];
## note that muhat = E(Y) and Vhat = Var(Y)
muhat <- apply(traindata[,3:202], 1, mean);
Vhat  <- apply(traindata[,3:202], 1, var)
data0 = data.frame(X1 = X1, X2=X2, muhat = muhat, Vhat = Vhat)
#tidy models learning stuff
set.seed(123)
#splitting data
split = initial_split(data0)
train = training(split)
test = testing(split)
set.seed(234)
folds = bootstraps(train)
library(splines)
# Recipe with polynomial features and interaction
Vhat_spline_rec <- recipe(Vhat ~ X1 + X2, data = train) %>%
step_poly(X1, degree = tune("x1_degree")) %>%  # Polynomial for X1
step_poly(X2, degree = tune("x2_degree")) %>%  # Polynomial for X2
step_interact(terms = ~ starts_with("X1"):starts_with("X2"))  # Interaction
# Model specification
lm_spec <- linear_reg() %>%
set_engine("lm")  # Use glmnet or stan if needed
# Workflow
Vhat_spline_wf <- workflow() %>%
add_recipe(Vhat_spline_rec) %>%
add_model(lm_spec)
# Tuning grid
spline_grid <- expand.grid(
x1_degree = c(1:15),
x2_degree = c(1:15)
)
# Register parallel backend
doParallel::registerDoParallel()
# Control for saving predictions
save_preds <- control_grid(save_pred = TRUE)
# Tuning
spline_vhat <- tune_grid(
Vhat_spline_wf,
resamples = folds,
grid = spline_grid,
control = save_preds
)
1.1085447^2
# Stop parallel processing
doParallel::stopImplicitCluster()
# Evaluate Results
collect_metrics(spline_vhat)
show_best(spline_vhat, metric = "rmse")
Best_Vhat = select_best(spline_vhat, metric = 'rmse')
Best_Vhat
Final_V = finalize_workflow(Vhat_spline_wf, Best_Vhat )
Final_V
V_spline_test = last_fit(Final_V, split)
collect_metrics(V_spline_test)
23.0624195^2
### Read Training Data
## Assume you save the training data in the folder "C:/temp" in your local laptop
traindata <- read.table(file = "C:/temp/7406train.csv", sep=",");
dim(traindata);
## dim=10000*202
## The first two columns are X1 and X2 values, and the last 200 columns are the Y valus
### Some example plots for exploratory data analysis
### please feel free to add more exploratory analysis
X1 <- traindata[,1];
X2 <- traindata[,2];
## note that muhat = E(Y) and Vhat = Var(Y)
muhat <- apply(traindata[,3:202], 1, mean);
Vhat  <- apply(traindata[,3:202], 1, var);
## You can construct a dataframe in R that includes all crucial
##    information for our exam
data0 = data.frame(X1 = X1, X2=X2, muhat = muhat, Vhat = Vhat);
## we can plot 4 graphs in a single plot
par(mfrow = c(2, 2));
plot(X1, muhat);
plot(X2, muhat);
plot(X1, Vhat);
plot(X2, Vhat);
library(rgl)
# 3D scatter plot for muhat
plot3d(data0$X1, data0$X2, data0$muhat,
type = 'p',
col.grid = 'grey',
col = gray(seq(0, 1, length.out = length(data0$muhat))),  # Grayscale palette,
size = 3,
xlab = "X1", ylab = "X2", zlab = "Mean (muhat)",
main = "")
# 3D scatter plot for Vhat
plot3d(data0$X1, data0$X2, data0$Vhat,
col = gray(seq(0, 1, length.out = length(data0$muhat))),
size = 3,
xlab = "X1", ylab = "X2", zlab = "        Variance (Vhat)",
main = "")
# 3D scatter plot for Vhat
plot3d(data0$X1, data0$X2, data0$Vhat,
col = gray(seq(0, 1, length.out = length(data0$muhat))),
size = 3,
xlab = "X1", ylab = "X2", zlab = "                   Variance (Vhat)",
main = "")
# 3D scatter plot for Vhat
plot3d(data0$X1, data0$X2, data0$Vhat,
col = gray(seq(1, 1, length.out = length(data0$muhat))),
size = 3,
xlab = "X1", ylab = "X2", zlab = "                   Variance (Vhat)",
main = "")
# 3D scatter plot for Vhat
plot3d(data0$X1, data0$X2, data0$Vhat,
col = gray(seq(0, 0, length.out = length(data0$muhat))),
size = 3,
xlab = "X1", ylab = "X2", zlab = "                   Variance (Vhat)",
main = "")
# 3D scatter plot for Vhat
plot3d(data0$X1, data0$X2, data0$Vhat,
col = gray(seq(0, .5, length.out = length(data0$muhat))),
size = 3,
xlab = "X1", ylab = "X2", zlab = "                   Variance (Vhat)",
main = "")
# 3D scatter plot for muhat
plot3d(data0$X1, data0$X2, data0$muhat,
type = 'p',
col.grid = 'grey',
col = gray(seq(0, .45, length.out = length(data0$muhat))),  # Grayscale palette,
size = 3,
xlab = "X1", ylab = "X2", zlab = "Mean (muhat)",
main = "")
# 3D scatter plot for Vhat
plot3d(data0$X1, data0$X2, data0$Vhat,
col = gray(seq(0, .45, length.out = length(data0$muhat))),
size = 3,
xlab = "X1", ylab = "X2", zlab = "                   Variance (Vhat)",
main = "")
# 3D scatter plot for Vhat
plot3d(data0$X1, data0$X2, data0$Vhat,
col = gray(seq(0, .45, length.out = length(data0$muhat))),
size = 3,
xlab = "X1", ylab = "X2", zlab = "       Variance (Vhat)",
main = "")
# 3D scatter plot for Vhat
plot3d(data0$X1, data0$X2, data0$Vhat,
col = gray(seq(0, .45, length.out = length(data0$muhat))),
size = 3,
xlab = "X1", ylab = "X2", zlab = "(Vhat)",
main = "")
plot3d(data0$X1, data0$X2, data0$muhat,
type = 'p',
col.grid = 'grey',
col = gray(seq(0, .45, length.out = length(data0$muhat))),  # Grayscale palette,
size = 3,
xlab = "X1", ylab = "X2", zlab = "(muhat)",
main = "")
Final_V
predicted_variance = predict(Final_V, new_data = testX)
best_params <- select_best(V_spline_test, metric = "rmse")
Final_V <- finalize_workflow(V_spline_wf, best_params)
Final_V = finalize_workflow(spline_vhat, Best_Vhat )
Final_V = finalize_workflow(Vhat_spline_wf, Best_Vhat )
Final_V
#Final_V <- finalize_workflow(V_spline_wf, best_params)
predicted_variance = predict(V_spline_test, new_data = testX)
?`extract-workflow`
Final_V <- extract_workflow(V_spline_wf)
Final_V <- extract_workflow(V_spline_test)
predicted_variance = predict(Final_V, new_data = testX)
Final_V <- extract_workflow(V_spline_test)
predicted_variance = predict(Final_V, new_data = testX)
predicted_variance
#setup multitheading
library(doParallel)
cores = detectCores()
library(dplyr)
library(tidymodels)
library(usemodels)
## Assume you save the training data in the folder "C:/temp" in your local laptop
traindata <- read.table(file = "C:/temp/7406train.csv", sep=",");
dim(traindata);
## dim=10000*202
## The first two columns are X1 and X2 values, and the last 200 columns are the Y valus
### Some example plots for exploratory data analysis
### please feel free to add more exploratory analysis
testX  <- read.table(file = "C:/temp/7406test.csv", sep=",")
names(traindata)[names(traindata) == 'V1'] = 'X1'
names(traindata)[names(traindata) == 'V2'] = 'X2'
names(testX)[names(testX) == 'V1'] = 'X1'
names(testX)[names(testX) == 'V2'] = 'X2'
X1 <- traindata[,1];
X2 <- traindata[,2];
## note that muhat = E(Y) and Vhat = Var(Y)
muhat <- apply(traindata[,3:202], 1, mean);
Vhat  <- apply(traindata[,3:202], 1, var)
data0 = data.frame(X1 = X1, X2=X2, muhat = muhat, Vhat = Vhat)
#tidy models learning stuff
set.seed(123)
#splitting data
split = initial_split(data0)
train = training(split)
test = testing(split)
set.seed(234)
folds = bootstraps(train)
library(splines)
# Recipe with polynomial features and interaction
muhat_spline_rec <- recipe(muhat ~ X1 + X2, data = train) %>%
step_poly(X1, degree = tune("x1_degree")) %>%  # Polynomial for X1
step_poly(X2, degree = tune("x2_degree")) %>%  # Polynomial for X2
step_interact(terms = ~ starts_with("X1"):starts_with("X2"))  # Interaction
# Model specification
lm_spec <- linear_reg() %>%
set_engine("glm")  # Use glmnet or stan if needed
# Workflow
muhat_spline_wf <- workflow() %>%
add_recipe(muhat_spline_rec) %>%
add_model(lm_spec)
# Tuning grid
spline_grid <- expand.grid(
x1_degree = c(1:10),
x2_degree = c(1:10)
)
# Register parallel backend
doParallel::registerDoParallel()
# Control for saving predictions
save_preds <- control_grid(save_pred = TRUE)
# Tuning
spline_mu <- tune_grid(
muhat_spline_wf,
resamples = folds,
grid = spline_grid,
control = save_preds
)
# Stop parallel processing
doParallel::stopImplicitCluster()
# Evaluate Results
collect_metrics(spline_mu)
Best_muhat = select_best(spline_mu, metric = 'rmse')
Best_muhat
Final_mu = finalize_workflow(muhat_spline_wf, Best_muhat )
Final_mu
mu_spline_test = last_fit(Final_mu, split)
collect_metrics(mu_spline_test)
Final_Mu_model <- extract_workflow(Final_mu)
Final_Mu_model <- extract_workflow(Final_mu)
Final_Mu_model <- extract_workflow(mu_spline_test)
predicted_mean = predict(Final_Mu_model, new_data = testX)
predicted_mean
testX
testX$muhat = predicted_mean
testX$muhat = predicted_mean
testX
Final_Mu_model <- extract_workflow(mu_spline_test)
predicted_mean = predict(Final_Mu_model, new_data = testX)
predicted_mean.pred
Final_Mu_model <- extract_workflow(mu_spline_test)
predicted_mean = predict(Final_Mu_model, new_data = testX)
predicted_mean$.pred
testX$muhat = predicted_mean$.pred
testX
Final_Mu_model <- extract_workflow(mu_spline_test)
predicted_mean = predict(Final_Mu_model, new_data = testX)
predicted_mean
testX
#tidy models learning stuff
set.seed(123)
#splitting data
split = initial_split(data0)
train = training(split)
test = testing(split)
set.seed(234)
folds = bootstraps(train)
library(splines)
# Recipe with polynomial features and interaction
Vhat_spline_rec <- recipe(Vhat ~ X1 + X2, data = train) %>%
step_poly(X1, degree = tune("x1_degree")) %>%  # Polynomial for X1
step_poly(X2, degree = tune("x2_degree")) %>%  # Polynomial for X2
step_interact(terms = ~ starts_with("X1"):starts_with("X2"))  # Interaction
# Model specification
lm_spec <- linear_reg() %>%
set_engine("lm")  # Use glmnet or stan if needed
# Workflow
Vhat_spline_wf <- workflow() %>%
add_recipe(Vhat_spline_rec) %>%
add_model(lm_spec)
# Tuning grid
spline_grid <- expand.grid(
x1_degree = c(1:15),
x2_degree = c(1:15)
)
# Register parallel backend
doParallel::registerDoParallel()
# Control for saving predictions
save_preds <- control_grid(save_pred = TRUE)
# Tuning
spline_vhat <- tune_grid(
Vhat_spline_wf,
resamples = folds,
grid = spline_grid,
control = save_preds
)
# Stop parallel processing
doParallel::stopImplicitCluster()
# Evaluate Results
collect_metrics(spline_vhat)
show_best(spline_vhat, metric = "rmse")
Best_Vhat = select_best(spline_vhat, metric = 'rmse')
Best_Vhat
Final_V = finalize_workflow(Vhat_spline_wf, Best_Vhat )
Final_V
V_spline_test = last_fit(Final_V, split)
collect_metrics(V_spline_test)
Final_V <- extract_workflow(V_spline_test)
predicted_variance = predict(Final_V, new_data = testX)
predicted_variance
```{r}
V_spline_test = last_fit(Final_V, split)
Best_Vhat = select_best(spline_vhat, metric = 'rmse')
Best_Vhat
Final_V = finalize_workflow(Vhat_spline_wf, Best_Vhat )
Final_V
Final_V = finalize_workflow(Vhat_spline_wf, Best_Vhat )
Final_V
V_spline_test = last_fit(Final_V, split)
collect_metrics(V_spline_test)
Final_V <- extract_workflow(V_spline_test)
predicted_variance = predict(Final_V, new_data = testX)
predicted_variance
testX
testX$vhat = predicted_variance.pred
testX$vhat = predicted_variance$.pred
testX
write.csv(testx, "predictions.csv", row.names = FALSE)
write.csv(testX, "predictions.csv", row.names = FALSE)
getwd()
# 3D scatter plot for muhat
plot3d(testX$X1, testX$X2, testX$muhat,
type = 'p',
col.grid = 'grey',
col = gray(seq(0, .45, length.out = length(data0$muhat))),  # Grayscale palette,
size = 3,
xlab = "X1", ylab = "X2", zlab = "(muhat)",
main = "")
library(rgl)
# 3D scatter plot for muhat
plot3d(testX$X1, testX$X2, testX$muhat,
type = 'p',
col.grid = 'grey',
col = gray(seq(0, .45, length.out = length(data0$muhat))),  # Grayscale palette,
size = 3,
xlab = "X1", ylab = "X2", zlab = "(muhat)",
main = "")
library(rgl)
# 3D scatter plot for muhat
plot3d(testX$X1, testX$X2, testX$muhat,
type = 'p',
col.grid = 'grey',
col = gray(seq(0, .45, length.out = length(testX$muhat))),  # Grayscale palette,
size = 3,
xlab = "X1", ylab = "X2", zlab = "(muhat)",
main = "")
library(rgl)
# 3D scatter plot for muhat
plot3d(testX$X1, testX$X2, testX$muhat,
type = 'p',
col.grid = 'grey',
col = gray(seq(0, .45, length.out = length(testX$muhat))),  # Grayscale palette,
size = 3,
xlab = "X1", ylab = "X2", zlab = "(muhat)",
main = "")
# 3D scatter plot for Vhat
plot3d(testX$X1, testX$X2, testX$vhat,
col = gray(seq(0, .45, length.out = length(testX$muhat))),
size = 3,
xlab = "X1", ylab = "X2", zlab = "(Vhat)",
main = "")
library(rgl)
# 3D scatter plot for muhat
plot3d(testX$X1, testX$X2, testX$muhat,
type = 'p',
col.grid = 'grey',
col = gray(seq(0, .45, length.out = length(testX$muhat))),  # Grayscale palette,
size = 3,
xlab = "X1", ylab = "X2", zlab = "(muhat)",
main = "")
Final_V$formula
Final_V$trained
Final_V$fit
